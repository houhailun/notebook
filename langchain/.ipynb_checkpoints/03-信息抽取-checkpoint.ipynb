{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cd8a22",
   "metadata": {},
   "source": [
    "# ä¿¡æ¯æŠ½å– Extraction\n",
    "- Extractionæ˜¯ä»ä¸€æ®µæ–‡æœ¬ä¸­è§£ææ•°æ®çš„è¿‡ç¨‹\n",
    "- é€šå¸¸ä¸Extraction parserä¸€èµ·ä½¿ç”¨ï¼Œä»¥æ„å»ºæ•°æ®\n",
    "\n",
    "1. ä»å¥å­ä¸­æå–ç»“æ„åŒ–è¡Œä»¥æ’å…¥æ•°æ®åº“\n",
    "2. ä»é•¿æ–‡æ¡£ä¸­æå–å¤šè¡Œä»¥æ’å…¥æ•°æ®åº“\n",
    "3. ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å‚æ•°ä»¥è¿›è¡ŒAPIè°ƒç”¨\n",
    "4. æœ€è¿‘æœ€ç«çš„Extractionåº“æ˜¯KOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9829307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here put the import lib\n",
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "import os\n",
    "\n",
    "# ç»§æ‰¿è‡ª langchain.llms.base.LLM\n",
    "class ZhipuAILLM(LLM):\n",
    "    # é»˜è®¤é€‰ç”¨ glm-3-turbo\n",
    "    model: str = \"glm-3-turbo\"\n",
    "    # æ¸©åº¦ç³»æ•°\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"acf4f9247da5e232fbe056b14b35fd9b.uWW0WvWqwWUYjhzQ\"\n",
    "    \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        client = ZhipuAI(\n",
    "            api_key = self.api_key\n",
    "        )\n",
    "\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "\n",
    "            è¯·æ±‚å‚æ•°ï¼š\n",
    "                prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "        \n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "\n",
    "    # é¦–å…ˆå®šä¹‰ä¸€ä¸ªè¿”å›é»˜è®¤å‚æ•°çš„æ–¹æ³•\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–è°ƒç”¨APIçš„é»˜è®¤å‚æ•°ã€‚\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c28b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ZhipuAILLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8257d0",
   "metadata": {},
   "source": [
    "## 1. æ‰‹åŠ¨æ ¼å¼è½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d625f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# è§£æè¾“å‡ºå¹¶è·å–ç»“æ„åŒ–çš„æ•°æ®\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2c84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Extraction\n",
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bce0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Python dictionary with the fruit names and their corresponding emojis:\n",
      "\n",
      "```python\n",
      "fruit_dict = {\n",
      "    'Apple': 'ğŸ',\n",
      "    'Pear': 'ğŸ',\n",
      "    'Kiwi': 'ğŸ¥'\n",
      "}\n",
      "```\n",
      "\n",
      "Note: I've included \"Kiwi\" as it was mentioned in your sentence. If you meant \"kiwi\" to be a part of a larger word (like \"this is an kiwi\"), please clarify, and I'll adjust the entry accordingly.\n"
     ]
    }
   ],
   "source": [
    "prompt = instructions + fruit_names\n",
    "\n",
    "# Call the LLM\n",
    "output = llm(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e962e1c",
   "metadata": {},
   "source": [
    "## è‡ªåŠ¨æ ¼å¼è½¬æ¢\n",
    "è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªå¸¦æœ‰æ ¼å¼è¯´æ˜çš„æç¤º\n",
    "\n",
    "è¿™æ ·å°±ä¸éœ€è¦æ‹…å¿ƒæç¤ºå·¥ç¨‹çš„é—®é¢˜äº†ï¼Œå°†è¿™éƒ¨åˆ†å®Œå…¨äº¤ç»™ Lang Chain æ¥æ‰§è¡Œ\n",
    "\n",
    "å°†LLMçš„è¾“å‡ºè½¬åŒ–ä¸º python å¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe090683",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# è§£æå™¨å°†ä¼šæŠŠLLMçš„è¾“å‡ºä½¿ç”¨æˆ‘å®šä¹‰çš„schemaè¿›è¡Œè§£æå¹¶è¿”å›æœŸå¾…çš„ç»“æ„æ•°æ®ç»™æˆ‘\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe15bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2a5a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™ä¸ª Prompt ä¸ä¹‹å‰æˆ‘ä»¬æ„å»º Chat Model æ—¶ Prompt ä¸åŒ\n",
    "# è¿™ä¸ª Prompt æ˜¯ä¸€ä¸ª ChatPromptTemplateï¼Œå®ƒä¼šè‡ªåŠ¨å°†æˆ‘ä»¬çš„è¾“å‡ºè½¬åŒ–ä¸º python å¯¹è±¡\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecc0a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "\n",
    "print (fruit_query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49cbb087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"artist\": \"Portugal. The Man\",\n",
      "\t\"song\": \"So Young\"\n",
      "}\n",
      "```\n",
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# fruit_output = llm(fruit_query.to_messages())\n",
    "\n",
    "# è‡ªå®šä¹‰çš„chatglmç±»ä¸openaiåœ¨ä½¿ç”¨ä¸Šæœ‰å·®åˆ«\n",
    "fruit_output = llm.invoke(fruit_query)\n",
    "print(fruit_output)\n",
    "\n",
    "output = output_parser.parse(fruit_output)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101c2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
