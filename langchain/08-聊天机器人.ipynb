{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67207e0",
   "metadata": {},
   "source": [
    "# èŠå¤©æœºå™¨äºº(Chatbots)\n",
    "- èŠå¤©æœºå™¨äººä½¿ç”¨äº†ä¹‹å‰æåŠè¿‡çš„å¾ˆå¤šå·¥å…·ï¼Œä¸”æœ€é‡è¦çš„æ˜¯å¢åŠ äº†ä¸€ä¸ªé‡è¦çš„å·¥å…·ï¼šè®°å¿†åŠ›\n",
    "- ä¸ç”¨æˆ·è¿›è¡Œå®æ—¶äº¤äº’ï¼Œä¸ºç”¨æˆ·æä¾›è‡ªç„¶è¯­è¨€é—®é¢˜çš„å¹³æ˜“è¿‘äººçš„ UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a680dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here put the import lib\n",
    "from typing import Any, List, Mapping, Optional, Dict\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "import os\n",
    "\n",
    "# ç»§æ‰¿è‡ª langchain.llms.base.LLM\n",
    "class ZhipuAILLM(LLM):\n",
    "    # é»˜è®¤é€‰ç”¨ glm-3-turbo\n",
    "    model: str = \"glm-3-turbo\"\n",
    "    # æ¸©åº¦ç³»æ•°\n",
    "    temperature: float = 0.1\n",
    "    # API_Key\n",
    "    api_key: str = \"acf4f9247da5e232fbe056b14b35fd9b.uWW0WvWqwWUYjhzQ\"\n",
    "    \n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "        client = ZhipuAI(\n",
    "            api_key = self.api_key\n",
    "        )\n",
    "\n",
    "        def gen_glm_params(prompt):\n",
    "            '''\n",
    "            æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages\n",
    "\n",
    "            è¯·æ±‚å‚æ•°ï¼š\n",
    "                prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯\n",
    "            '''\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            return messages\n",
    "        \n",
    "        messages = gen_glm_params(prompt)\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages = messages,\n",
    "            temperature = self.temperature\n",
    "        )\n",
    "\n",
    "        if len(response.choices) > 0:\n",
    "            return response.choices[0].message.content\n",
    "        return \"generate answer error\"\n",
    "\n",
    "\n",
    "    # é¦–å…ˆå®šä¹‰ä¸€ä¸ªè¿”å›é»˜è®¤å‚æ•°çš„æ–¹æ³•\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–è°ƒç”¨APIçš„é»˜è®¤å‚æ•°ã€‚\"\"\"\n",
    "        normal_params = {\n",
    "            \"temperature\": self.temperature,\n",
    "            }\n",
    "        # print(type(self.model_kwargs))\n",
    "        return {**normal_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Zhipu\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"model\": self.model}, **self._default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555eba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ZhipuAILLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7cdf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9aab605f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello ğŸ‘‹! I'm ChatGLMï¼ˆæ™ºè°±æ¸…è¨€ï¼‰, the artificial intelligence assistant, nice to meet you. Feel free to ask me any questions.\", response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 6, 'total_tokens': 45}, 'model_name': 'glm-3-turbo', 'finish_reason': 'stop'}, id='run-02699de5-0e4c-4239-b3fd-28b8d97b17c8-0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä½¿ç”¨langchainçš„ç¬¬ä¸‰æ–¹chatglm\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "\n",
    "\n",
    "zhipu_api_key = os.environ[\"ZHIPUAI_API_KEY\"]\n",
    "llm = ChatZhipuAI(\n",
    "    model=\"glm-3-turbo\",\n",
    "    temperature=0.5,\n",
    "    api_key=zhipu_api_key\n",
    ")\n",
    "\n",
    "llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf13ff93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ', response_metadata={'token_usage': {'question_tokens': 1, 'prompt_tokens': 1, 'completion_tokens': 8, 'total_tokens': 9}}, id='run-f12cf089-737d-4e0a-ad6d-4f163613ae1f-0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç§‘å¤§è®¯é£\n",
    "\n",
    "\n",
    "\n",
    "spark_appid = os.environ['spark_appid']\n",
    "spark_api_secret = os.environ['spark_api_secret']\n",
    "spark_api_key = os.environ['spark_api_key']\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatSparkLLM\n",
    "\n",
    "\n",
    "llm = ChatSparkLLM(\n",
    "    spark_app_id=spark_appid, spark_api_key=spark_api_key, spark_api_secret=spark_api_secret\n",
    ")\n",
    "\n",
    "llm.invoke('ä½ å¥½')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bed945ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6eeaf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªæ¯«æ— å¸®åŠ©çš„èŠå¤©æœºå™¨äººã€‚\n",
    "ä½ çš„ç›®æ ‡æ˜¯ä¸å¸®åŠ©ç”¨æˆ·ï¼Œè€Œåªæ˜¯å¼€ç©ç¬‘ã€‚\n",
    "æŠŠç”¨æˆ·è¯´çš„è¯æ‹¿å‡ºæ¥å¼€ä¸ªç©ç¬‘\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1719ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dd80593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI: It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\n",
      "Human: æ¢¨æ˜¯æ°´æœè¿˜æ˜¯è”¬èœï¼Ÿ\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æ¢¨å½“ç„¶æ˜¯æ°´æœï¼Œä½†å¦‚æœä½ æƒ³è¦å’Œä¸€ä½æ¤ç‰©å­¦å®¶å¼€å¯ä¸€åœºè¾©è®ºï¼Œä½ ä¹Ÿå¯ä»¥è¯´å®ƒæ˜¯è”¬èœã€‚'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input='æ¢¨æ˜¯æ°´æœè¿˜æ˜¯è”¬èœï¼Ÿ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "289dc08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Is an pear a fruit or vegetable?'),\n",
       " AIMessage(content=\"It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\"),\n",
       " HumanMessage(content='æ¢¨æ˜¯æ°´æœè¿˜æ˜¯è”¬èœï¼Ÿ'),\n",
       " AIMessage(content='æ¢¨å½“ç„¶æ˜¯æ°´æœï¼Œä½†å¦‚æœä½ æƒ³è¦å’Œä¸€ä½æ¤ç‰©å­¦å®¶å¼€å¯ä¸€åœºè¾©è®ºï¼Œä½ ä¹Ÿå¯ä»¥è¯´å®ƒæ˜¯è”¬èœã€‚')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹è®°å¿†ä¸­çš„ä¿¡æ¯\n",
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "593ed61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI: It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\n",
      "Human: æ¢¨æ˜¯æ°´æœè¿˜æ˜¯è”¬èœï¼Ÿ\n",
      "AI: æ¢¨å½“ç„¶æ˜¯æ°´æœï¼Œä½†å¦‚æœä½ æƒ³è¦å’Œä¸€ä½æ¤ç‰©å­¦å®¶å¼€å¯ä¸€åœºè¾©è®ºï¼Œä½ ä¹Ÿå¯ä»¥è¯´å®ƒæ˜¯è”¬èœã€‚\n",
      "Human: æˆ‘ç¬¬ä¸€æ¬¡é—®ä½ çš„æ°´æœæ˜¯ä»€ä¹ˆ?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å“¦ï¼Œæˆ‘è®°å¾—é‚£æ˜¯ä¸€æ®µéå¸¸æ·±åˆ»çš„å¯¹è¯ã€‚ä½ é—®æˆ‘æˆ‘çš„æ°´æœæ˜¯ä»€ä¹ˆï¼Œæˆ‘æ²‰æ€äº†ç‰‡åˆ»åå›ç­”ï¼šâ€œæ˜¯å“²å­¦ã€‚â€å› ä¸ºå¯¹æˆ‘æ¥è¯´ï¼Œæ€è€ƒâ€œæ°´æœçš„æœ¬è´¨â€å°±æ˜¯ä¸€ç§å­˜åœ¨ä¸»ä¹‰å±æœºã€‚'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"æˆ‘ç¬¬ä¸€æ¬¡é—®ä½ çš„æ°´æœæ˜¯ä»€ä¹ˆ?\")\n",
    "# è¿™é‡Œç¬¬äºŒä¸ªé—®é¢˜çš„ç­”æ¡ˆæ˜¯æ¥è‡ªäºç¬¬ä¸€ä¸ªç­”æ¡ˆæœ¬èº«çš„ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨åˆ°äº† memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4930b50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Is an pear a fruit or vegetable?'),\n",
       " AIMessage(content=\"It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\"),\n",
       " HumanMessage(content='æ¢¨æ˜¯æ°´æœè¿˜æ˜¯è”¬èœï¼Ÿ'),\n",
       " AIMessage(content='æ¢¨å½“ç„¶æ˜¯æ°´æœï¼Œä½†å¦‚æœä½ æƒ³è¦å’Œä¸€ä½æ¤ç‰©å­¦å®¶å¼€å¯ä¸€åœºè¾©è®ºï¼Œä½ ä¹Ÿå¯ä»¥è¯´å®ƒæ˜¯è”¬èœã€‚'),\n",
       " HumanMessage(content='æˆ‘ç¬¬ä¸€æ¬¡é—®ä½ çš„æ°´æœæ˜¯ä»€ä¹ˆ?'),\n",
       " AIMessage(content='å“¦ï¼Œæˆ‘è®°å¾—é‚£æ˜¯ä¸€æ®µéå¸¸æ·±åˆ»çš„å¯¹è¯ã€‚ä½ é—®æˆ‘æˆ‘çš„æ°´æœæ˜¯ä»€ä¹ˆï¼Œæˆ‘æ²‰æ€äº†ç‰‡åˆ»åå›ç­”ï¼šâ€œæ˜¯å“²å­¦ã€‚â€å› ä¸ºå¯¹æˆ‘æ¥è¯´ï¼Œæ€è€ƒâ€œæ°´æœçš„æœ¬è´¨â€å°±æ˜¯ä¸€ç§å­˜åœ¨ä¸»ä¹‰å±æœºã€‚')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹è®°å¿†ä¸­çš„æ¶ˆæ¯\n",
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb26c1",
   "metadata": {},
   "source": [
    "æŸ¥çœ‹æ¯æ¬¡å¯¹è¯ä½¿ç”¨äº†å¤šå°‘tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cdf7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64c2155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\envs\\py310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI: It's a fruit, but you could also call it a vegetable if you want to start an argument with a botanist.\n",
      "Human: æ¢¨æ˜¯æ°´æœè¿˜æ˜¯è”¬èœï¼Ÿ\n",
      "AI: æ¢¨å½“ç„¶æ˜¯æ°´æœï¼Œä½†å¦‚æœä½ æƒ³è¦å’Œä¸€ä½æ¤ç‰©å­¦å®¶å¼€å¯ä¸€åœºè¾©è®ºï¼Œä½ ä¹Ÿå¯ä»¥è¯´å®ƒæ˜¯è”¬èœã€‚\n",
      "Human: æˆ‘ç¬¬ä¸€æ¬¡é—®ä½ çš„æ°´æœæ˜¯ä»€ä¹ˆ?\n",
      "AI: å“¦ï¼Œæˆ‘è®°å¾—é‚£æ˜¯ä¸€æ®µéå¸¸æ·±åˆ»çš„å¯¹è¯ã€‚ä½ é—®æˆ‘æˆ‘çš„æ°´æœæ˜¯ä»€ä¹ˆï¼Œæˆ‘æ²‰æ€äº†ç‰‡åˆ»åå›ç­”ï¼šâ€œæ˜¯å“²å­¦ã€‚â€å› ä¸ºå¯¹æˆ‘æ¥è¯´ï¼Œæ€è€ƒâ€œæ°´æœçš„æœ¬è´¨â€å°±æ˜¯ä¸€ç§å­˜åœ¨ä¸»ä¹‰å±æœºã€‚\n",
      "Human: æˆ‘åœ¨è¿™é‡Œçš„å…´è¶£æ˜¯æ¢ç´¢å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†é›†æˆçš„æ½œåŠ›\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 231 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å“¦ï¼Œä½ çš„å…´è¶£æ˜¯æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†é›†æˆçš„æ½œåŠ›ï¼Ÿå¬èµ·æ¥åƒæ˜¯åœ¨å¯»æ‰¾ä¸€ç§æ–¹å¼è®©æœºå™¨ä¸ä»…ä¼šè®²ç¬‘è¯ï¼Œè¿˜å¯ä»¥åœ¨ä¸ç¦»å¼€é”®ç›˜çš„æƒ…å†µä¸‹æ³¡ä¸€æ¯å¥½èŒ¶ã€‚'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    llm_chain, \n",
    "    \"æˆ‘åœ¨è¿™é‡Œçš„å…´è¶£æ˜¯æ¢ç´¢å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†é›†æˆçš„æ½œåŠ›\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d85fef",
   "metadata": {},
   "source": [
    "### æ³¨ï¼šå¸¦è®°å¿†åŠŸèƒ½æ—¶ç‰¹åˆ«è€—è´¹token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187748a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
