{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b634f8",
   "metadata": {},
   "source": [
    "# 引言\n",
    "\n",
    "分词器是每个大语言模型必不可少的组件，但每个大语言模型的分词器几乎都不相同。如果要训练自己的分词器，可以使用huggingface的tokenizers框架，tokenizers包含以下主要组件：\n",
    "\n",
    "- Tokenizer: 分词器的核心组件，定义了分词的整个流程，包括标准化、预分词、模型分词、后处理等\n",
    "- Normalizers：可选，负责将文本标准化，包括unicode归一化、大写转小写、去重音等操作\n",
    "- Pre-tokenizers：负责将文本分割成更小的片段（如单词等），为模型分词做准备。常见的预分词器有按空格分词（Whitespace）、正则表达式分词（Regex）等\n",
    "- Models：是实际的分词算法，负责将文本片段转换为子词，常见的有BPE、WordPiece、Unigram等。\n",
    "- Post-Processors：负责对分词结果进行后处理，如添加特殊标记（CLS、SEP）。\n",
    "- Decoders：负责将分词结果转换回原始文本，常见的解码器有 ByteLevel、WordPiece 等。\n",
    "- Trainers：用于训练分词模型，不同的模型对应不同的训练器，如 BpeTrainer、WordPieceTrainer、UnigramTrainer 等。\n",
    "\n",
    "在开始之前，先导入对应的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cf8ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    models,\n",
    "    processors,\n",
    "    decoders,\n",
    "    trainers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79757c",
   "metadata": {},
   "source": [
    "# 加载语料库\n",
    "\n",
    "这里的语料库是jsonl文件，每一行是一条json个格式的文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abea82",
   "metadata": {},
   "source": [
    "定义一个函数用于从JSONL文件中读取文本数据，考虑到语料库会比较大，所以采用yield生成器来延迟到访问时再加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5dcf36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_from_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as fr:\n",
    "        for i, line in enumerate(fr):\n",
    "#             if i == 0: print(line)\n",
    "            if i >= 100000: break\n",
    "            data = json.loads(line)\n",
    "            yield data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f3f6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"./datasets/pretrain_hq.jsonl\"\n",
    "texts = read_texts_from_jsonl(data_path)\n",
    "type(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ad013",
   "metadata": {},
   "source": [
    "可以看到，函数read_texts_from_jsonl返回的并不是真正的数据，而是一个生成器generator。可以通过next函数像访问iterator一样访问迭代数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2432153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。</s> <s>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？</s> <s>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。</s> <s>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。</s> <s>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。</s> <s>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。</s> <s>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。</s> <s>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\\n文本：“今天是我的生日！”这个文本的语气是喜悦。</s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859a198",
   "metadata": {},
   "source": [
    "# 训练过程\n",
    "## 模型选择\n",
    "使用BPE模型来初始化Tokenizer实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a146d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb59f8c",
   "metadata": {},
   "source": [
    "BPE是一种基于子词的分词方法，例如：\n",
    "\n",
    "- cats -> cat + s\n",
    "- helpful -> help + ful\n",
    "- congratulation -> con + gr + at + ulation\n",
    "\n",
    "这种基于子词的分词方法，相比基于完整单词和基于单个字符有以下好处：\n",
    "\n",
    "1. 子词相比于单词（可以认为多个子词的组合）数量要可控，这能避免词表过大，并且能避免生僻词带来的未知令牌问题。\n",
    "2. 子词相比于字符语义性更强，像单个字符f是没有语义的，但子词ful可以表达满的，比较像英语里的词根词缀。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59b401",
   "metadata": {},
   "source": [
    "## 预分词器选择\n",
    "为tokenizer设置预分词器，预分词器有以下几种：\n",
    "- Whitespace：按空格分隔，粒度为单词，适用于空格分隔的语言，例如英语。\n",
    "- Regex：按自定义正则表达式分隔，适用于需要自定义复杂分词规则的场景。\n",
    "- ByteLevel：按字节分割，适用于特殊字符、非英语场景（例如中文）。\n",
    "\n",
    "由于我们主要面向中文，所以这里采用ByteLevel的pre_tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500e0da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.pre_tokenizers.ByteLevel at 0x217b69417b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8242d80",
   "metadata": {},
   "source": [
    "pre_tokenizer对文本做了哪些处理，通过以下几个例子查看"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad751be",
   "metadata": {},
   "source": [
    "1. 处理英文文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc15f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pre', (0, 3)),\n",
       " ('-', (3, 4)),\n",
       " ('tokenize', (4, 12)),\n",
       " ('Ġa', (12, 14)),\n",
       " ('Ġ:', (14, 16)),\n",
       " ('class', (16, 21)),\n",
       " (':`~', (21, 24)),\n",
       " ('tokenizers', (24, 34)),\n",
       " ('.', (34, 35)),\n",
       " ('PyPreTokenizedString', (35, 55)),\n",
       " ('`', (55, 56)),\n",
       " ('Ġin', (56, 59)),\n",
       " ('-', (59, 60)),\n",
       " ('place', (60, 65))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Pre-tokenize a :class:`~tokenizers.PyPreTokenizedString` in-place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d9d79",
   "metadata": {},
   "source": [
    "可以看到，pre_tokenizer将文本按照空格和特殊字符作了初步分词，空格处理成了特殊字符Ġ，并记录了每个词的起始和结束位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b1f02",
   "metadata": {},
   "source": [
    "2. 处理中文文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68621879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('åľ¨æŁ¥å¤ĦèĻļå¼Ģå¢ŀåĢ¼ç¨İä¸ĵçĶ¨åıĳç¥¨æ¡Īä»¶ä¸Ń', (0, 15)),\n",
       " ('ï¼Į', (15, 16)),\n",
       " ('å¸¸å¸¸æ¶īåıĬè¿Ľé¡¹çķĻæĬµç¨İé¢ĿåĴĮç¨İæ¬¾æįŁå¤±çļĦè®¤å®ļåĴĮå¤ĦçĲĨ', (16, 37)),\n",
       " ('ãĢĤ', (37, 38))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_sentence = \"在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。\"\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(zh_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72464e6b",
   "metadata": {},
   "source": [
    "中文基本也是按照特殊符号，和。进行了分词，但分词的结果是一堆不认识的字符，这些字符是如何产生的呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40232e42",
   "metadata": {},
   "source": [
    "## 构建训练器\n",
    "BPE训练器中需要指定几个参数：\n",
    "- vocab_size: 训练后词表中的词条数据，BPE是一个从短词到长词的组合过程，达到词表大小后就会停止训练。\n",
    "- special_tokens:特殊token，和语言模型的特殊token一样。例如开始、结束、填充\n",
    "- initial_alphabet:初始化字符表，使用上面长度为256的unicode字节编码表作为初始字符表。\n",
    "\n",
    "通过pre_tokenizers.ByteLevel.alphabet()可以获得初始字符编码表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49626d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"!\", \")\", \"Ü\", \"ı\", \"ĺ\", \"ú\", \"Ķ\", \"y\", \"M\", \"(\", \"ä\", \"¿\", \"à\", \"®\", \"ô\", \"ì\", \"ÿ\", \"0\", \"í\", \"ñ\", \"ĩ\", \"Ä\", \"ç\", \"Ĳ\", \"Ę\", \"´\", \"º\", \"¨\", \"]\", \"Ď\", \"Â\", \"ý\", \"±\", \"¯\", \"_\", \"Ĕ\", \"Ń\", \"9\", \"\\\\\"\", \"¬\", \"¶\", \"ĝ\", \"Ğ\", \"Ì\", \"Ĵ\", \"\\\\\\\\\", \"Č\", \"t\", \"`\", \"l\", \"đ\", \"m\", \"\\'\", \"Ł\", \"v\", \"ĥ\", \"¹\", \"2\", \"¼\", \"©\", \"ê\", \"ī\", \"c\", \"3\", \"Ú\", \"Ą\", \"ġ\", \"g\", \"ã\", \"ĭ\", \"è\", \"Ĝ\", \"$\", \"ĕ\", \"Ľ\", \"Õ\", \"Á\", \"Ĭ\", \"»\", \"Å\", \"ð\", \".\", \"Y\", \"P\", \"Ā\", \"8\", \"h\", \"µ\", \"C\", \"Ć\", \"ē\", \"ł\", \"5\", \"F\", \"w\", \"<\", \"I\", \"Ġ\", \"i\", \"Þ\", \"Ù\", \"1\", \"³\", \"Ĉ\", \"Ô\", \"B\", \"s\", \"x\", \"ĉ\", \"þ\", \"R\", \"û\", \"S\", \"Ċ\", \"&\", \"T\", \"ć\", \"|\", \"ą\", \"õ\", \"Ò\", \"Ē\", \"{\", \"Ç\", \",\", \"?\", \"V\", \"¦\", \"¾\", \"*\", \"×\", \"ŀ\", \"î\", \"ě\", \"W\", \"ė\", \"Ã\", \"É\", \"ö\", \"Ð\", \"E\", \"d\", \"ù\", \"§\", \"â\", \"÷\", \"Î\", \"H\", \"Ê\", \"j\", \"#\", \"e\", \"u\", \"Z\", \"Ñ\", \"é\", \"}\", \"Ļ\", \"Į\", \"İ\", \"U\", \"Ħ\", \"ľ\", \"G\", \"¤\", \"·\", \"K\", \"ò\", \"ï\", \":\", \"N\", \"å\", \"Ï\", \"/\", \"č\", \"D\", \"ă\", \"ĳ\", \"¥\", \"¸\", \"+\", \"ª\", \"ß\", \"~\", \"n\", \"Ĺ\", \"ģ\", \"ĵ\", \"a\", \"ó\", \"ë\", \"«\", \"ċ\", \"%\", \"@\", \"į\", \"4\", \"æ\", \"r\", \"-\", \"Ĥ\", \"Ģ\", \"Ă\", \"7\", \"À\", \"ħ\", \"k\", \"Ė\", \"Q\", \"p\", \"q\", \"ļ\", \"Ī\", \"^\", \"Ö\", \"ķ\", \"A\", \"X\", \">\", \"á\", \"Đ\", \"ď\", \"ü\", \"Ë\", \";\", \"²\", \"Í\", \"z\", \"=\", \"O\", \"ĸ\", \"Ø\", \"J\", \"Ĩ\", \"ę\", \"[\", \"ğ\", \"Ě\", \"¡\", \"ā\", \"f\", \"Ŀ\", \"½\", \"L\", \"b\", \"6\", \"£\", \"ø\", \"Ý\", \"Û\", \"È\", \"¢\", \"Ó\", \"Æ\", \"°\", \"o\"]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(pre_tokenizers.ByteLevel.alphabet(), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbbdd87",
   "metadata": {},
   "source": [
    "定义特殊token，分别为填充、开始、结束标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5df41310",
   "metadata": {},
   "outputs": [],
   "source": [
    "speicial_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa2274",
   "metadata": {},
   "source": [
    "构建训练器，词条数量设置为32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177c950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=speicial_tokens,\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b061c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dce210",
   "metadata": {},
   "source": [
    "# 保存训练结果\n",
    "\n",
    "在保存结果之前，需要先设置相匹配的解码器，否则ASCII以外的字符可能无法正常解码。\n",
    "\n",
    "> 上面编码阶段使用了ByteLevel的预分词器，相对应的解码阶段也需要使用ByteLevel，表示将token id转换为token后，还需要进行一次unicode字节级别的解码，才能正常显示中文等多语言字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2eced51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aad0b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizer\\\\vocab.json', './tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将训练的分词器保存到指定目录。\n",
    "\n",
    "tokenizer_dir = \"./tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6301bba",
   "metadata": {},
   "source": [
    "还需要一个分词器配置文件，包括模型类型、是否使用小写字母等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a35224b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": True,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<|endoftext|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"content\": \"<|im_start|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"content\": \"<|im_end|>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "        }\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<|im_start|>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"<|im_end|>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 1000000000000000019884624838656,\n",
    "    \"pad_token\": None,\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"use_default_system_prompt\": False,\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e1b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer training completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a60cc",
   "metadata": {},
   "source": [
    "1. vocab.json：词汇表文件，包含词条和对应的索引。\n",
    "2. merges.txt: 合并表文件，定义了子词的合并规则。\n",
    "3. tokenizer.json: 完整的分词器文件，它包含了分词器的所有信息，包括词汇表、合并规则、特殊标记等。\n",
    "4. tokenizer_config.json: 分词器配置文件，包括了起始token、结束token的定义，以及提示词模板。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f826e66",
   "metadata": {},
   "source": [
    "# 测试分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ea6cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer_dir = \"./tokenizer\"\n",
    "tokenizer_trained = AutoTokenizer.from_pretrained(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd62f2",
   "metadata": {},
   "source": [
    "1. 英文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1633e772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P',\n",
       " 're',\n",
       " '-',\n",
       " 't',\n",
       " 'ok',\n",
       " 'en',\n",
       " 'i',\n",
       " 'z',\n",
       " 'e',\n",
       " 'Ġ',\n",
       " 'a',\n",
       " 'Ġ',\n",
       " ':',\n",
       " 'c',\n",
       " 'l',\n",
       " 'as',\n",
       " 's',\n",
       " ':',\n",
       " '`',\n",
       " '~',\n",
       " 't',\n",
       " 'ok',\n",
       " 'en',\n",
       " 'i',\n",
       " 'z',\n",
       " 'er',\n",
       " 's',\n",
       " '.',\n",
       " 'P',\n",
       " 'y',\n",
       " 'P',\n",
       " 're',\n",
       " 'T',\n",
       " 'ok',\n",
       " 'en',\n",
       " 'i',\n",
       " 'z',\n",
       " 'e',\n",
       " 'd',\n",
       " 'S',\n",
       " 't',\n",
       " 'r',\n",
       " 'ing',\n",
       " '`',\n",
       " 'Ġ',\n",
       " 'in',\n",
       " '-',\n",
       " 'p',\n",
       " 'l',\n",
       " 'a',\n",
       " 'ce']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_en = \"Pre-tokenize a :class:`~tokenizers.PyPreTokenizedString` in-place\"\n",
    "tokenized = tokenizer_trained.tokenize(text_en)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a217d6",
   "metadata": {},
   "source": [
    "tokenize方法只对输入文本作了分词，返回的是一组明文的token。要想直接返回token_id，需要使用encode方法，分词的同时完成文本到数字的序列化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d89b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50,\n",
       " 27224,\n",
       " 15,\n",
       " 86,\n",
       " 29472,\n",
       " 16889,\n",
       " 75,\n",
       " 92,\n",
       " 71,\n",
       " 223,\n",
       " 67,\n",
       " 223,\n",
       " 28,\n",
       " 69,\n",
       " 78,\n",
       " 22605,\n",
       " 85,\n",
       " 28,\n",
       " 66,\n",
       " 96,\n",
       " 86,\n",
       " 29472,\n",
       " 16889,\n",
       " 75,\n",
       " 92,\n",
       " 13757,\n",
       " 85,\n",
       " 16,\n",
       " 50,\n",
       " 91,\n",
       " 50,\n",
       " 27224,\n",
       " 54,\n",
       " 29472,\n",
       " 16889,\n",
       " 75,\n",
       " 92,\n",
       " 71,\n",
       " 70,\n",
       " 53,\n",
       " 86,\n",
       " 84,\n",
       " 31833,\n",
       " 66,\n",
       " 223,\n",
       " 12359,\n",
       " 15,\n",
       " 82,\n",
       " 78,\n",
       " 67,\n",
       " 13717]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_en = tokenizer_trained.encode(text_en)\n",
    "token_ids_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74c8f368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pre-tokenize a :class:`~tokenizers.PyPreTokenizedString` in-place'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_trained.decode(token_ids_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30841a",
   "metadata": {},
   "source": [
    "2. 测试中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "028587a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[304,\n",
       " 906,\n",
       " 670,\n",
       " 2315,\n",
       " 637,\n",
       " 1230,\n",
       " 1444,\n",
       " 4134,\n",
       " 1665,\n",
       " 384,\n",
       " 491,\n",
       " 1772,\n",
       " 6552,\n",
       " 305,\n",
       " 263,\n",
       " 3740,\n",
       " 3160,\n",
       " 472,\n",
       " 848,\n",
       " 1577,\n",
       " 4823,\n",
       " 4134,\n",
       " 2636,\n",
       " 301,\n",
       " 4134,\n",
       " 1592,\n",
       " 6922,\n",
       " 265,\n",
       " 1096,\n",
       " 382,\n",
       " 14332,\n",
       " 262]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_zh = \"在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。\"\n",
    "token_ids_zh = tokenizer_trained.encode(text_zh)\n",
    "token_ids_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50786c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_trained.decode(token_ids_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78523f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792a864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f32d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
